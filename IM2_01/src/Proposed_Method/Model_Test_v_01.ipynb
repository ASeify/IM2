{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GKcIHTZK0QrQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_PATH = os.path.dirname(os.path.abspath('D:/Masters thesis/Code/Classes/'))\n",
    "if not (CLASSES_PATH in sys.path):\n",
    "    sys.path.append(CLASSES_PATH)\n",
    "from Classes.Files_Handler_Class import Files_Handler\n",
    "from Classes.K_Shell_Calculate_Class import K_Shell_Calculate\n",
    "from Classes.Resd_Network_Infos_Class import Resd_Network_Infos\n",
    "from Classes.SIR_Diffusion_Model_Class import SIR_Diffusion_Model\n",
    "from Classes.Get_Past_Results_Class import Get_Past_Results\n",
    "from Classes.Network_Infos_Writer_Class import Network_Infos_Writer\n",
    "from Classes.Layers_Ranking_Class_Old import Layers_Ranking\n",
    "from Classes.Network_Node_Centrality_Class_Old import Network_Node_Centrality\n",
    "from Classes.Bcolors_Class import Bcolors as bcolors\n",
    "from Classes.Average_Meter_Class import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_set_size = 5\n",
    "version_num = 'v01'\n",
    "\n",
    "color_list = [\"light_red\", \"light_green\", \"light_yellow\",\n",
    "               \"light_blue\",\"light_magenta\", \"light_cyan\",\n",
    "               \"blue\", \"red\", \"white\", \"green\", \"yellow\",\n",
    "                 \"magenta\", \"cyan\", ]\n",
    "tqdm_color_list = ['blue', 'red', 'green', 'cyan', 'magenta', 'yellow', 'black', 'white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_centrality = ['layer_density','layer_degree_histogram','layer_edge_weight',\n",
    "                    'layer_sombor_index', 'layer_nodes_weight','layer_k_shell_weight']\n",
    "node_centrality = ['degree', 'clustering', 'nip', 'sombor_index', 'ego_density','ego_degree',\n",
    "                     'ego_k_shell', 'ego_degree_mean','kss', 'vote_power']\n",
    "drop_centrality = ['layer_id', 'node_id', 'weight', 'k_shell', 'k_shell_itr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1fwFbjlz0QrT"
   },
   "outputs": [],
   "source": [
    "source_code_path = str(os.getcwd())\n",
    "source_code_path = source_code_path.replace(\"\\\\\", \"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5pZ-tpz0QrT",
    "outputId": "87bd3c86-0b78-48bc-9997-7baed3af6c55"
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "File Selection Canceled !",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m File Selection Canceled !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python 3.11.2\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "files_handler_obj = Files_Handler()\n",
    "file_path = files_handler_obj.select_files(\"text files\", \".edgeslist .edgelist .edges .mtx .txt\")\n",
    "if file_path is None or file_path == '':\n",
    "    sys.exit(\"File Selection Canceled !\")\n",
    "file_info = files_handler_obj.get_file_path_info(file_path)\n",
    "network_name = file_info['name']\n",
    "network_type = file_info['type']\n",
    "network_path = file_info['path']\n",
    "if network_name == \"\":\n",
    "    sys.exit(\"Dont Network Selection!\")\n",
    "file_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GsCNPeaQ0QrU"
   },
   "outputs": [],
   "source": [
    "resd_network_infos_object = Resd_Network_Infos()\n",
    "(\n",
    "    network_layers_info,\n",
    "    network_layers_nodes,\n",
    "    entra_layer_edges,\n",
    "    entra_layer_edges_features,\n",
    "    inter_layer_edge,\n",
    ") = Resd_Network_Infos.read_nodeFrom_layerFrom_nodeTo_layerTo(\n",
    "    network_path, network_name, network_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sr2Arb-X0QrU",
    "outputId": "e8e8fb99-407d-418a-b7c9-3f6576ff0902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanremo2016_final \n",
      "\n",
      "\u001b[92mLayer 1: 49904 Node And 210308 Edge\u001b[0m\n",
      "\u001b[93mLayer 2: 34564 Node And 92970 Edge\u001b[0m\n",
      "\u001b[94mLayer 3: 9461 Node And 11253 Edge\u001b[0m\n",
      "\n",
      "network entier nodes : \u001b[33m55897\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(file_info['name'], '\\n')\n",
    "layers_id = []\n",
    "network_layers_count = len(network_layers_info)\n",
    "graphs_of_network = [None] * network_layers_count\n",
    "network_entier_edges = \"\"\n",
    "layers_nodes_infect_scale = []\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while i < network_layers_count:\n",
    "    graphs_of_network[i] = nx.Graph()\n",
    "    network_layers_nodes[i] = list(set(network_layers_nodes[i]))\n",
    "    layers_nodes_infect_scale.append({})\n",
    "    if len(network_layers_nodes[i]) > 0:\n",
    "        graphs_of_network[i].add_edges_from(entra_layer_edges[i])\n",
    "\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"degree\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"k_shell\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"k_shell_itr\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"nip\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"sombor_index\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"ego_density\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"ego_degree\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"ego_k_shell\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"ego_degree_mean\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"kss\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], 1, \"vote_power\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"clustering\")\n",
    "        nx.set_node_attributes(graphs_of_network[i], None, \"SIR\")\n",
    "        \n",
    "        graphs_of_network[i].graph[\"id\"] = i\n",
    "        graphs_of_network[i].graph[\"layer_density\"] = nx.density(graphs_of_network[i])\n",
    "        graphs_of_network[i].graph[\"layer_degree_histogram\"] = None\n",
    "        graphs_of_network[i].graph[\"layer_edge_weight\"] = None\n",
    "        graphs_of_network[i].graph[\"layer_sombor_index\"] = None\n",
    "        graphs_of_network[i].graph[\"layer_nodes_weight\"] = None\n",
    "        graphs_of_network[i].graph[\"layer_k_shell_weight\"] = None\n",
    "\n",
    "        K_Shell_Calculate_Object = K_Shell_Calculate(graphs_of_network[i])\n",
    "        graphs_of_network[i] = K_Shell_Calculate_Object.get_k_shell_info()\n",
    "        del K_Shell_Calculate_Object\n",
    "        degrees = dict(graphs_of_network[i].degree())\n",
    "        nx.set_node_attributes(graphs_of_network[i], degrees, \"degree\")\n",
    "        layers_id.append(str(i))\n",
    "        print(colored(\"Layer \"  + str(i) + \": \" + str(graphs_of_network[i].number_of_nodes()) + \" Node And \" +\n",
    "                       str(graphs_of_network[i].number_of_edges()) + \" Edge\", color_list[j]))\n",
    "        # print(colored(graphs_of_network[i].graph['k_shell_info'], color_list[i]))\n",
    "    i += 1\n",
    "    j += 1\n",
    "    if j >= len(color_list):\n",
    "        j = 0\n",
    "\n",
    "network_entier_nodes_list = []\n",
    "for item in network_layers_nodes:\n",
    "    network_entier_nodes_list += item\n",
    "\n",
    "network_entier_nodes_list = list(set(network_entier_nodes_list))\n",
    "network_entier_nodes_count = len(network_entier_nodes_list)\n",
    "print()\n",
    "print(\"network entier nodes : \" + colored(str(network_entier_nodes_count), \"yellow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_past_results_obj = Get_Past_Results(network_path, network_name)\n",
    "network_infos_writer_object = Network_Infos_Writer(network_path, network_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model_path = files_handler_obj.make_dir(file_info['path'], 'our_model_' + version_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Me7kYlpD0QrV"
   },
   "outputs": [],
   "source": [
    "del network_layers_nodes, entra_layer_edges\n",
    "del entra_layer_edges_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bx8E343H0QrX",
    "outputId": "1695a6a2-2286-493c-d62d-3c71ef611ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanremo2016_final\n",
      "Load layer results:  \u001b[95m\u001b[91mFalse\u001b[0m\n",
      "\u001b[95m\u001b[94mCalc layer layers_density_weight\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ? Layer/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 4: 100%|██████████| 4/4 [00:00<00:00, 29.00 Layer/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[94m\n",
      "Calc layer layers_degree_distribution\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 4: 100%|██████████| 4/4 [00:00<00:00, 29.87Layer/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[94m\n",
      "Calc layer layers_edges_and_sombor_index\u001b[0m\n",
      "\u001b[95mLayer 1: Graph with 49904 nodes and 210308 edges\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge 210308: 100%|██████████| 210308/210308 [04:52<00:00, 719.39 Edge/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mLayer 2: Graph with 34564 nodes and 92970 edges\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge 92970: 100%|██████████| 92970/92970 [02:20<00:00, 662.93 Edge/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mLayer 3: Graph with 9461 nodes and 11253 edges\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Edge 11253: 100%|██████████| 11253/11253 [00:18<00:00, 596.41 Edge/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[94m\n",
      "Calc layer layers_nodes_weight\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 4: 100%|██████████| 4/4 [00:00<00:00, 800.52Layer/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[94m\n",
      "Calc layer layers_k_shell_weight\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer 4: 100%|██████████| 4/4 [00:00<00:00, 174.02Layer/s] \n"
     ]
    }
   ],
   "source": [
    "print(network_name)\n",
    "layer_past_result_status = False\n",
    "layer_past_result_status = get_past_results_obj.get_past_layer_att_results(graphs_of_network)\n",
    "if layer_past_result_status:\n",
    "        print(\"Load layer results: \", bcolors.HEADER + bcolors.OKGREEN + str(layer_past_result_status) + bcolors.ENDC)\n",
    "else:\n",
    "        print(\"Load layer results: \", bcolors.HEADER + bcolors.FAIL + str(layer_past_result_status) + bcolors.ENDC)\n",
    "if not layer_past_result_status:\n",
    "        layers_ranking_object = Layers_Ranking()\n",
    "        print(bcolors.HEADER + bcolors.OKBLUE + 'Calc layer layers_density_weight' + bcolors.ENDC)\n",
    "        layers_ranking_object.layers_density_weight(graphs_of_network)\n",
    "        print(bcolors.HEADER + bcolors.OKBLUE + '\\nCalc layer layers_degree_distribution' + bcolors.ENDC)\n",
    "        layers_ranking_object.layers_degree_distribution(graphs_of_network)\n",
    "        print(bcolors.HEADER + bcolors.OKBLUE + '\\nCalc layer layers_edges_and_sombor_index' + bcolors.ENDC)\n",
    "        layers_ranking_object.layers_edges_and_sombor_index(graphs_of_network)\n",
    "        print(bcolors.HEADER + bcolors.OKBLUE + '\\nCalc layer layers_nodes_weight' + bcolors.ENDC)\n",
    "        layers_ranking_object.layers_nodes_weight(graphs_of_network)\n",
    "        print(bcolors.HEADER + bcolors.OKBLUE + '\\nCalc layer layers_k_shell_weight' + bcolors.ENDC)\n",
    "        layers_ranking_object.layers_k_shell_weight(graphs_of_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not layer_past_result_status:\n",
    "    network_infos_writer_object.write_network_layer_infos_csv(graphs_of_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20A79kSa0QrX",
    "outputId": "6a83a10a-beda-4d20-f3be-1952752a330f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "past_node_att_file = get_past_results_obj.get_past_node_att_file()\n",
    "if past_node_att_file != False:\n",
    "    node_past_result_att_status = float(past_node_att_file.split('/')[-1].split(' ')[-2].split('=')[-1])\n",
    "else:\n",
    "    node_past_result_att_status = 0\n",
    "print(node_past_result_att_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BbToPeE0QrX",
    "outputId": "e18e9b79-8d6c-4f61-b231-9d6ec594e3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer\t density\t degree_histogram\t edge_weight\t sombor_index\t nodes_weight\t k_shell_weight\n",
      "\u001b[92m1 \t 0.0001689 \t 8.4285027 \t\t 2.1030800 \t 44.1067673 \t 4.9904000 \t 5.5497844\u001b[0m\n",
      "\u001b[93m2 \t 0.0001556 \t 5.3795857 \t\t 0.9297000 \t 100.9348857 \t 3.4564000 \t 1.8012175\u001b[0m\n",
      "\u001b[94m3 \t 0.0002515 \t 2.3788183 \t\t 0.1125300 \t 10.3024962 \t 0.9461000 \t 0.8879464\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print('Layer\\t density\\t degree_histogram\\t edge_weight\\t sombor_index\\t nodes_weight\\t k_shell_weight')\n",
    "f_p = '{:9.7f}' # float padding\n",
    "c = 0\n",
    "for i, graph in enumerate(graphs_of_network):\n",
    "    if graph.number_of_nodes() > 0:\n",
    "        print(colored((str(i)+ ' \\t ' + str(f_p.format(graph.graph[\"layer_density\"]))+ ' \\t ' +\n",
    "                       str(f_p.format(graph.graph[\"layer_degree_histogram\"]))+ ' \\t\\t ' +\n",
    "                       str(f_p.format(graph.graph[\"layer_edge_weight\"]))+ ' \\t ' +\n",
    "                       str(f_p.format(graph.graph[\"layer_sombor_index\"]))+ ' \\t ' +\n",
    "                       str(f_p.format(graph.graph[\"layer_nodes_weight\"]))+ ' \\t ' +\n",
    "                       str(f_p.format(graph.graph[\"layer_k_shell_weight\"]))),\n",
    "                       color_list[c]))\n",
    "    c += 1\n",
    "    if c >= len(color_list):\n",
    "        c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "f08KHwph0QrZ",
    "outputId": "8f4f8424-a883-4d76-cf86-b56d34288523"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnetwork_name\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# past_results_status = False\u001b[39;00m\n\u001b[0;32m      3\u001b[0m past_results_status, past_results_nodes \u001b[38;5;241m=\u001b[39m get_past_results_obj\u001b[38;5;241m.\u001b[39mget_past_node_att_results(graphs_of_network)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'network_name' is not defined"
     ]
    }
   ],
   "source": [
    "print(network_name)\n",
    "# past_results_status = False\n",
    "past_results_status, past_results_nodes = get_past_results_obj.get_past_node_att_results(graphs_of_network)\n",
    "if past_results_status:\n",
    "    print(\"\\nLoad temp results: \", bcolors.HEADER + bcolors.OKGREEN + str(past_results_status) + bcolors.ENDC)\n",
    "    print(\"Load temp results count: \", bcolors.HEADER + bcolors.OKGREEN + str(len(past_results_nodes)) + bcolors.ENDC, '\\n')\n",
    "else:\n",
    "    print(\"Load temp results: \", bcolors.HEADER + bcolors.FAIL + str(past_results_status) + bcolors.ENDC, '\\n')\n",
    "Network_Node_Centrality_obj = Network_Node_Centrality()\n",
    "hop_num = 2\n",
    "Network_Node_Centrality_obj.get_nodes_centrality(network_infos_writer_object, graphs_of_network, None, hop=hop_num)\n",
    "network_infos_writer_object.write_network_nodes_info_csv(1, graphs_of_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nzFGXKrg0QrZ",
    "outputId": "4faa0706-759f-4316-bc8e-2eac14dc9e31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HIabTV9t0QrZ"
   },
   "outputs": [],
   "source": [
    "class Multilayer_Full_Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, node_in_features: int, node_out_features: int,\n",
    "        layer_in_features: int, layer_out_features: int,\n",
    "        encoder_head: int, num_encoder:int, encoder_activation: str,\n",
    "        bias: bool, dropout: float,\n",
    "        activation: nn.modules.activation, device: str = \"cpu\",\n",
    "        h0:int = 8, h1: int = 16, h2: int = 32, h3: int = 64, h4: int = 128, h5 = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.node_embeding = nn.Sequential(\n",
    "            nn.Linear(in_features=node_in_features, out_features=h1, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h1, out_features=h2, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h2, out_features=h3, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h3, out_features=h4, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h4, out_features=node_out_features, bias=bias, device=device)\n",
    "        )\n",
    "        self.layer_embeding = nn.Sequential(\n",
    "            nn.Linear(in_features=layer_in_features, out_features=h1, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h1, out_features=h2, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h2, out_features=h3, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h3, out_features=h4, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h4, out_features=node_out_features, bias=bias, device=device)\n",
    "            )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=node_out_features, nhead=encoder_head,\n",
    "                dim_feedforward=(4 * node_out_features), dropout=dropout,\n",
    "                activation=encoder_activation, bias=bias,\n",
    "                batch_first=True, device=device),\n",
    "            num_encoder)\n",
    "        self.regression = nn.Sequential(\n",
    "            nn.Linear(in_features=node_out_features, out_features=h4, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h4, out_features=h3, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h3, out_features=h2, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h2, out_features=h1, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h1, out_features=h0, bias=bias, device=device),\n",
    "            activation,\n",
    "            nn.Linear(in_features=h0, out_features=1, bias=bias, device=device),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, node_x, layer_x):\n",
    "        node_y = self.node_embeding(node_x).unsqueeze(dim=2)\n",
    "        layer_y = self.layer_embeding(layer_x).unsqueeze(dim=1)\n",
    "        y = torch.matmul(node_y, layer_y)\n",
    "        y = self.encoder(y)\n",
    "        y = torch.mean(y, dim=1)\n",
    "        y = self.regression(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HNUKSzG90QrZ",
    "outputId": "d6d66689-bf5c-472f-f4df-695c6dbc04b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'highest_epoch_train model lr=0.0001 wd=1e-05 epochs=82  loss_valid=7.3802 loss_train=7.2628.pt'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = ''\n",
    "files_names = files_handler_obj.get_files_in_path(source_code_path)\n",
    "for item in files_names:\n",
    "    if item.split('.')[-1] == 'pt':\n",
    "        model_file = item\n",
    "        break\n",
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2iuRsE-Q0Qra",
    "outputId": "bbb0e09c-605e-425c-8cb3-37fddf2867a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mModel load\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_path = source_code_path + '/' +model_file\n",
    "model = None\n",
    "try:\n",
    "    model = torch.load(model_path, map_location=torch.device(device))\n",
    "    torch.set_grad_enabled(False)\n",
    "    print(bcolors.OKGREEN + 'Model load' + bcolors.ENDC)\n",
    "except Exception as e:\n",
    "    print(bcolors.FAIL + 'Model not found!' + bcolors.ENDC)\n",
    "    sys.exit(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Lq6Aeq230Qra",
    "outputId": "b44d844c-d57b-4242-ccd7-d8f8adf881d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " True\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "network_nodes_list_SIR_p = {}\n",
    "nodes_SIR_p = defaultdict(dict)\n",
    "SIR_p_file_status = False\n",
    "SIR_p_file = file_info['path'] + file_info['name'] + ' SIR_p_' + version_num +'/' + file_info['name'] + ' SIR_p_' + version_num + '.csv'\n",
    "try:\n",
    "    with open(SIR_p_file) as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        network_nodes_list_SIR_p = dict(reader)\n",
    "    SIR_p_file_status = True\n",
    "except:\n",
    "    SIR_p_file_status = False\n",
    "\n",
    "nodes_SIR_p_file_status = False\n",
    "nodes_SIR_p_file = file_info['path'] + file_info['name'] + ' SIR_p_' + version_num + '/' + file_info['name'] + ' temp SIR_p_' + version_num  + '.csv'\n",
    "try:\n",
    "    nodes_SIR_p = pd.read_csv(nodes_SIR_p_file, index_col=0)\n",
    "    nodes_SIR_p = pd.DataFrame.to_dict(nodes_SIR_p, orient=\"index\")\n",
    "    nodes_SIR_p_file_status = True\n",
    "except:\n",
    "    nodes_SIR_p_file_status = False\n",
    "\n",
    "print(SIR_p_file_status, nodes_SIR_p_file_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Mf05DE8Q0Qrb"
   },
   "outputs": [],
   "source": [
    "if nodes_SIR_p_file_status:\n",
    "    for graph in graphs_of_network:\n",
    "        if graph.number_of_nodes() > 0 and graph.number_of_edges() > 0:\n",
    "            nx.set_node_attributes(graph, None, \"SIR_p_\" + version_num)\n",
    "    for node in network_entier_nodes_list:\n",
    "        node_SIR_p_values = nodes_SIR_p[int(node)]\n",
    "        for k, v in node_SIR_p_values.items():\n",
    "            if k != 'AVG':\n",
    "                if node in graphs_of_network[int(k)].nodes():\n",
    "                    graphs_of_network[int(k)].nodes[node]['SIR_p_' + version_num]= float(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, test_loader:DataLoader,\n",
    "               loss_fn:nn.modules.activation, epoch:int=None, device:str='cuda'):\n",
    "  model.eval()\n",
    "  with tqdm(test_loader, unit=\" batch\") as tepoch:\n",
    "    with torch.no_grad():\n",
    "      loss_valid = AverageMeter()\n",
    "      # acc_valid = Accuracy().to(device)\n",
    "      for node_inputs, layer_inputs, targets in tepoch:\n",
    "        if epoch is not None:\n",
    "          tepoch.set_description(f\"Test  Epoch {epoch + 1}\")\n",
    "        node_inputs = node_inputs.to(device)\n",
    "        layer_inputs = layer_inputs.to(device)\n",
    "        # targets = targets.to(device)\n",
    "\n",
    "        outputs = model(node_inputs, layer_inputs).squeeze(dim=1)\n",
    "        loss = loss_fn(outputs.to(device), targets.to(device))\n",
    "\n",
    "        loss_valid.update(loss.item())\n",
    "        tepoch.set_postfix(loss=loss_valid.avg)\n",
    "        # acc_valid(outputs, targets.int())\n",
    "  return loss_valid.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rKVk4vc_0Qra"
   },
   "outputs": [],
   "source": [
    "def get_model_outputs(model, node_data_loader:DataLoader, layer_data:torch.Tensor, device):\n",
    "  model.eval()\n",
    "  outputs = []\n",
    "  with tqdm(node_data_loader, unit=\" batch\") as tepoch:\n",
    "    with torch.no_grad():\n",
    "      for i, (node_inputs, _) in enumerate(tepoch):\n",
    "        tepoch.set_description(f\"Batch {i + 1}\")\n",
    "        node_inputs = node_inputs.to(device)\n",
    "        layer_inputs = layer_data.to(device)\n",
    "        outputs += (((model(node_inputs, layer_inputs)).squeeze(dim=1)).tolist())\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_mu = torch.FloatTensor([3.3446e-03, 1.1519e-01, 2.0886e-07, 1.8509e-10, 1.0834e-07, 6.8439e-10, 1.1653e-09, 2.0585e-06, 1.9182e-05, 1.0586e-03])\n",
    "node_std = torch.FloatTensor([8.5905e-03, 1.8260e-01, 1.0475e-06, 1.0219e-09, 5.7295e-07, 1.9116e-08, 1.3211e-08, 5.4032e-06, 1.0904e-04, 6.2570e-03])\n",
    "\n",
    "layer_mu = torch.FloatTensor([8.9085e-04, 2.3435e+00, 1.4069e+00, 4.8727e-01, 2.6765e+01, 7.9508e-01])\n",
    "layer_std = torch.FloatTensor([2.5558e-03, 2.8222e+00, 1.4067e+00, 1.4440e+00, 2.2386e+01, 1.7470e+00])\n",
    "\n",
    "node_scale_vector = torch.FloatTensor([10000, 1000000, 1000000000, 100000, 1000000000, 1000000000, 1000000, 100000, 10000, 1])\n",
    "layer_scale_vector = torch.FloatTensor([1, 10, 10, 100, 1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_sir = []\n",
    "model_sir = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6WZZIkvV0Qrb",
    "outputId": "c3a34cc4-4b81-4b90-c7c2-af9e3a906154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "Done.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "layers_counter = len(graphs_of_network)\n",
    "if not nodes_SIR_p_file_status and len(network_entier_nodes_list) != len(network_nodes_list_SIR_p.keys()):\n",
    "    tqdm_color_index = 0\n",
    "    for graph in graphs_of_network:\n",
    "        if graph.number_of_nodes() > 0 and graph.number_of_edges() > 0:\n",
    "            degrees = dict(graph.degree())  # Get the degree of each node\n",
    "            mean_degree = sum(degrees.values()) / len(degrees)\n",
    "            print(bcolors.FAIL + f'\\nlayer {layers_counter}: {graph}' + bcolors.ENDC)\n",
    "            real_sir.append([])\n",
    "            model_sir.append([])\n",
    "        # ---------- Extract layer features ---------------------\n",
    "            layer_x_data = []\n",
    "            layer_info = graph.graph\n",
    "            layer_id = layer_info['id']\n",
    "            for k, v in layer_info.items():\n",
    "                if (k in layer_centrality):\n",
    "                        layer_x_data.append(v)\n",
    "            \n",
    "            layer_x_data = torch.FloatTensor(layer_x_data)\n",
    "            layer_x_data = layer_x_data / layer_scale_vector\n",
    "            layer_x_data = (layer_x_data - layer_mu) / layer_std\n",
    "            layer_x_data = layer_x_data.to(device)\n",
    "        # -------------------------------------------------------\n",
    "        # ---------- Extract nodes features ---------------------\n",
    "            nodes_list = list(graph.nodes())\n",
    "            with tqdm(nodes_list, unit=\" Node\") as tepoch:\n",
    "                if tqdm_color_index >= len(tqdm_color_list):\n",
    "                    tqdm_color_index = 0\n",
    "                tepoch.colour = tqdm_color_list[tqdm_color_index]\n",
    "                tqdm_color_index += 1\n",
    "                for i, node in enumerate(tepoch):\n",
    "                    tepoch.set_description(f\"Node {i + 1}\")\n",
    "                    node_x_data = []\n",
    "                    node_info = graph.nodes[node]\n",
    "                    for centrality in node_centrality:\n",
    "                        node_x_data.append(float(node_info[centrality]))\n",
    "            # -------------------------------------------------------\n",
    "                    node_x_data = torch.FloatTensor(node_x_data)\n",
    "                    node_x_data = node_x_data / node_scale_vector\n",
    "                    node_x_data = (node_x_data - node_mu) / node_std\n",
    "                    node_x_data = node_x_data.to(device)\n",
    "                # -----Feeding data to neural network and get outputs----\n",
    "                    with torch.no_grad():\n",
    "                        output = model(node_x_data.unsqueeze(0), layer_x_data.unsqueeze(0)).squeeze(1).tolist()[0]\n",
    "                    \n",
    "                    output = output * (graphs_of_network[layer_id].degree(node) / mean_degree)\n",
    "                    graphs_of_network[layer_id].nodes[node][\"SIR_p_\" + version_num] = output\n",
    "                    nodes_SIR_p[node][layer_id] = output\n",
    "                    real_sir[-1].append(graphs_of_network[layer_id].nodes[node][\"SIR\"])\n",
    "                    model_sir[-1].append(output)\n",
    "        # -------------------------------------------------------\n",
    "        layers_counter -= 1\n",
    "else:\n",
    "    for graph in graphs_of_network:\n",
    "        if graph.number_of_nodes() > 0 and graph.number_of_edges() > 0:\n",
    "            real_sir.append([])\n",
    "            model_sir.append([])\n",
    "            layer_id = graph.graph['id']\n",
    "            nodes_list = list(graph.nodes())\n",
    "            for node in nodes_list:\n",
    "                real_sir[-1].append(graphs_of_network[layer_id].nodes[node][\"SIR\"])\n",
    "                model_sir[-1].append(graphs_of_network[layer_id].nodes[node][\"SIR_p_\" + version_num])\n",
    "\n",
    "print(bcolors.OKGREEN + '\\nDone.' + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WO1701FL0Qrb"
   },
   "outputs": [],
   "source": [
    "if not nodes_SIR_p_file_status:\n",
    "    path_SIR_p =  files_handler_obj.make_dir(file_info['path'], file_info['name']+' SIR_p_' + version_num)\n",
    "    write_layers_SIR_p_status = network_infos_writer_object.write_layers_SIR_p_to_csv(path_SIR_p, file_info['name']+\" temp SIR_p_\" + version_num, nodes_SIR_p)\n",
    "    if write_layers_SIR_p_status == 'Done.':\n",
    "        print(bcolors.OKGREEN + write_layers_SIR_p_status + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: nan\n",
      "Layer 2: nan\n",
      "Layer 3: nan\n",
      "Layer 4: nan\n",
      "Layer 5: nan\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.HuberLoss()\n",
    "for i, item in enumerate(model_sir):\n",
    "    loss = loss_fn(torch.FloatTensor(real_sir[i]), torch.FloatTensor(model_sir[i]))\n",
    "    print(f\"Layer {i + 1}: {loss.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "85SdnG9I0Qrc",
    "outputId": "67e42106-155e-4e30-80a0-24690d8ac8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "node_by_node_SIR_p_writer_status = SIR_p_file_status\n",
    "SIR_p_opened_file = None\n",
    "print(len(network_nodes_list_SIR_p.keys()))\n",
    "node_SIR_p_write_counter = 0\n",
    "nodes_SIR_p_value = {}\n",
    "if len(network_entier_nodes_list) != len(network_nodes_list_SIR_p.keys()):\n",
    "    node_counter = len(network_nodes_list_SIR_p.keys())\n",
    "    print(bcolors.FAIL + 'calculating global SIR_p_' + version_num + ':' + bcolors.ENDC)\n",
    "    with tqdm(network_entier_nodes_list, unit=\" Node\") as t_nodes:\n",
    "        t_nodes.colour=tqdm_color_list[4]\n",
    "        for node in t_nodes:\n",
    "            if node not in network_nodes_list_SIR_p:\n",
    "                t_nodes.set_description(f\"Node {node_counter}\")\n",
    "                node_counter += 1\n",
    "                node_SIR_p = {}\n",
    "                for graph in graphs_of_network:\n",
    "                    if graph.number_of_nodes() > 0 and graph.number_of_edges() > 0 and node in graph.nodes():\n",
    "                        layer_id = graph.graph['id']\n",
    "                        node_SIR_p[layer_id] = float(graph.nodes[node]['SIR_p_' + version_num])\n",
    "                # node_SIR_p = {k: v for k, v in sorted(node_SIR_p.items(),key=lambda item: item[1], reverse=True)}\n",
    "                # SIR_p = None\n",
    "                SIR_p = 0\n",
    "                # common_neighbors = set()\n",
    "                for i, (k_layer_id, v) in enumerate(node_SIR_p.items()):\n",
    "                #     if SIR_p is None:\n",
    "                #         SIR_p = v\n",
    "                #         common_neighbors = set(list(graphs_of_network[k_layer_id].neighbors(node)))\n",
    "                #     else:\n",
    "                #         node_neighbors = set(list(graphs_of_network[k_layer_id].neighbors(node)))\n",
    "                #         # node_neighborhood_effect = len(node_neighbors - common_neighbors) len(node_neighbors)\n",
    "                #         node_neighborhood_effect = (len(node_neighbors) - len(node_neighbors - common_neighbors)) * (i * 0.01)\n",
    "                #         common_neighbors = set.union(common_neighbors, node_neighbors)\n",
    "                #         # if node_neighborhood_effect < 0.25:\n",
    "                #         #     node_neighborhood_effect = 0.25\n",
    "                #         SIR_p += (v - node_neighborhood_effect)\n",
    "                    SIR_p += v\n",
    "                network_nodes_list_SIR_p[node] = SIR_p\n",
    "                nodes_SIR_p_value[node] = SIR_p\n",
    "                node_SIR_p_write_counter += 1\n",
    "                if node_SIR_p_write_counter >= 1000 or len(network_nodes_list_SIR_p.keys()) == network_entier_nodes_count:\n",
    "                    node_by_node_SIR_p_writer_status = network_infos_writer_object.node_SIR_p_writer(node_by_node_SIR_p_writer_status, SIR_p_file, nodes_SIR_p_value)\n",
    "                    nodes_SIR_p_value = {}\n",
    "                    node_SIR_p_write_counter = 0\n",
    "    network_nodes_list_SIR_p\n",
    "if not SIR_p_opened_file is None:\n",
    "    SIR_p_opened_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "wWwx0BH30Qrc"
   },
   "outputs": [],
   "source": [
    "if not SIR_p_file_status:\n",
    "    network_nodes_list_SIR_p = {k: v for k, v in sorted(network_nodes_list_SIR_p.items(),\n",
    "                                                          key=lambda item: item[1], reverse=True)}\n",
    "# network_nodes_list_SIR_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "ZkSRUc200Qrc"
   },
   "outputs": [],
   "source": [
    "if not SIR_p_file_status:\n",
    "   import pandas as pd\n",
    "   SIR_p_file_root = files_handler_obj.make_dir(file_info['path'], file_info['name'] + ' SIR_p_' + version_num)\n",
    "   SIR_p_file_info = SIR_p_file_root + file_info['name'] + ' SIR_p_' + version_num + '.csv'\n",
    "   (pd.DataFrame.from_dict(data=network_nodes_list_SIR_p, orient='index')\n",
    "      .to_csv(SIR_p_file_info, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_nodes_list_SIR_p = list(network_nodes_list_SIR_p.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "ML-HyWaJ0Qrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set size: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['28', '52', '23', '46', '44']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Seed set size: {seed_set_size}\")\n",
    "i = 0\n",
    "seed_set = []\n",
    "for candidate_node, SIR_p in network_nodes_list_SIR_p:\n",
    "    if len(seed_set) < seed_set_size:\n",
    "        if i == 0:\n",
    "            seed_set.append(candidate_node)\n",
    "        else:\n",
    "            # seed_set.append(candidate_node)\n",
    "            common_neighbors_len = 0\n",
    "            node_neighbors_len = 0\n",
    "            for seed_node in seed_set:\n",
    "                for graph in graphs_of_network:\n",
    "                    if seed_node in graph and candidate_node in graph:\n",
    "                        common_neighbors_len += len(nx.common_neighbors(graph, seed_node, candidate_node))\n",
    "                        node_neighbors_len += graph.degree[candidate_node]\n",
    "            if node_neighbors_len > 0:\n",
    "                common_neighbors_percentage = common_neighbors_len / node_neighbors_len\n",
    "            else: \n",
    "                common_neighbors_percentage = 0\n",
    "            beta_scale = i * 0.01\n",
    "            if beta_scale > 0.5:\n",
    "                beta_scale = 0.5\n",
    "            candidate_node_efficient_SIR_p = float(SIR_p) - ((float(SIR_p) * common_neighbors_percentage) * beta_scale)\n",
    "            # print(SIR_p, common_neighbors_percentage, '\\n', beta_scale, candidate_node_efficient_SIR_p, '\\n')\n",
    "            if i < (len(network_nodes_list_SIR_p) - 1):\n",
    "                if candidate_node_efficient_SIR_p >= float(network_nodes_list_SIR_p[i + 1][1]):\n",
    "                    seed_set.append(candidate_node)\n",
    "            else:\n",
    "                if candidate_node_efficient_SIR_p <= 0.25:\n",
    "                    seed_set.append(candidate_node)\n",
    "    else:\n",
    "        break\n",
    "    i += 1\n",
    "    if i >= len(network_nodes_list_SIR_p):\n",
    "        for candidate_node, SIR_p in network_nodes_list_SIR_p:\n",
    "            if len(seed_set) < seed_set_size:\n",
    "                if not candidate_node in seed_set:\n",
    "                    seed_set.append(candidate_node)\n",
    "\n",
    "\n",
    "seed_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "g6fBwqL40Qrr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network name: Relationshipsfromsurveys\n",
      "Seed set size: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infect scale 0.84103: 100%|\u001b[32m██████████\u001b[0m| 10000/10000 [01:18<00:00, 126.88 Iteration/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network entier nodes count: 84\n",
      "Infected nodes count: 70.6465\n",
      "Percentage of infection: 0.841029761904762\n"
     ]
    }
   ],
   "source": [
    "print(f\"Network name: {network_name}\")\n",
    "print(f\"Seed set size: {seed_set_size}\")\n",
    "beta = 0.01\n",
    "landa = 0.7\n",
    "epoch = 10000\n",
    "SIR_diffusion_model_obj = SIR_Diffusion_Model()\n",
    "infection = SIR_diffusion_model_obj.synchronous_SIR_multilayer_with_seed_set_model(graphs_of_network, seed_set, beta, landa, epoch, network_entier_nodes_list)\n",
    "\n",
    "sir_results_infos = {}\n",
    "sir_results_infos['infection'] = infection\n",
    "sir_results_infos['percentage'] = infection / network_entier_nodes_count\n",
    "sir_results_infos['seed_set'] = seed_set\n",
    "\n",
    "network_infos_writer_object.write_results_in_file(our_model_path, f'infection k={seed_set_size} beta={beta} landa={landa} epoch={epoch}', sir_results_infos)\n",
    "print(f\"Network entier nodes count: {network_entier_nodes_count}\")\n",
    "print(f\"Infected nodes count: {infection}\")\n",
    "print(f\"Percentage of infection: {sir_results_infos['percentage']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
